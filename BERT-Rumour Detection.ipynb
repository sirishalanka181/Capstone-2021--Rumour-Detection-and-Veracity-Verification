{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"BERT-Rumour Detection.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"qgmaODodeHq9","executionInfo":{"status":"error","timestamp":1616929043422,"user_tz":-330,"elapsed":3122,"user":{"displayName":"Sirisha Lanka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghzwtsi42lroWnDz7GH_4C2lL1451TrdZhncayKLQ=s64","userId":"14732814864657524762"}},"outputId":"ed5be74d-4b08-41af-a468-13d5724929a9","colab":{"base_uri":"https://localhost:8080/","height":378}},"source":["import os\n","import json\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.metrics import classification_report\n","\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","import torch\n","import torch.nn as nn\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","def get_dataset():\n","    path=\"D:/Courses/VII Sem/Capstone/PHEME_veracity/PHEME_veracity/all-rnr-annotated-threads\"\n","    raw_data={\"id\":[],\"text\":[],\"label\":[]}\n","    files=os.listdir(path)\n","    events=[]\n","    for file in files:\n","        if os.path.isdir(os.path.join(os.path.abspath(path), file)):\n","            events.append(file)\n","    #print(events)\n","    for event in events:\n","        event_path_rumours=\"D:/Courses/VII Sem/Capstone/PHEME_veracity/PHEME_veracity/all-rnr-annotated-threads/\"+event+\"/rumours\"\n","        rumour_files=os.listdir(event_path_rumours)\n","        rumour_dirs=[]\n","        for f in rumour_files:\n","            if os.path.isdir(os.path.join(os.path.abspath(event_path_rumours), f)):\n","                rumour_dirs.append(f)\n","        for rum in rumour_dirs:\n","            file=open(\"D:/Courses/VII Sem/Capstone/PHEME_veracity/PHEME_veracity/all-rnr-annotated-threads/\"+event+\"/rumours/\"+rum+\"/annotation.json\")\n","            data=json.load(file)\n","            raw_data[\"id\"].append(rum)\n","            raw_data[\"text\"].append(data[\"category\"])\n","            raw_data[\"label\"].append(1)\n","        #print(raw_data)\n","\n","        event_path_nonrumours=\"D:/Courses/VII Sem/Capstone/PHEME_veracity/PHEME_veracity/all-rnr-annotated-threads/\"+event+\"/non-rumours\"\n","        non_rumour_files=os.listdir(event_path_nonrumours)\n","        non_rumour_dirs=[]\n","        for f in non_rumour_files:\n","            if os.path.isdir(os.path.join(os.path.abspath(event_path_nonrumours), f)):\n","                non_rumour_dirs.append(f)\n","        for nrum in non_rumour_dirs:\n","            file=open(\"D:/Courses/VII Sem/Capstone/PHEME_veracity/PHEME_veracity/all-rnr-annotated-threads/\"+event+\"/non-rumours/\"+nrum+\"/source-tweets/\"+nrum+\".json\")\n","            data=json.load(file)\n","            raw_data[\"id\"].append(nrum)\n","            raw_data[\"text\"].append(data[\"text\"])\n","            raw_data[\"label\"].append(0)\n","        \n","    \n","    X_train, X_test, y_train, y_test = train_test_split(raw_data['text'], raw_data['label'], test_size=0.3, random_state=2018,stratify=raw_data['label'])\n","        \n","    return X_train, X_test, y_train, y_test        \n","\n","\n","def drop_emoji(x):\n","    drop_list = []\n","    for i,_x in enumerate(x):\n","        if ord(_x) >= 128:\n","            drop_list.append(i)\n","    new_x = []\n","    for i,_x in enumerate(x):\n","        if i not in drop_list:\n","            new_x.append(_x)\n","    return ''.join(new_x)\n","\n","train_text,temp_text, train_labels, temp_labels = get_dataset()\n","\n","val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n","                                                                random_state=2018, \n","                                                                test_size=0.5, \n","                                                                stratify=temp_labels)\n","\n","\n","test_text=list(map(drop_emoji,test_text))\n","val_text=list(map(drop_emoji,val_text))\n","train_text=list(map(drop_emoji,train_text))\n","\n","bert = AutoModel.from_pretrained('bert-base-uncased')\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","seq_len = [len(i.split()) for i in train_text]\n","\n","tokens_train = tokenizer.batch_encode_plus(\n","    train_text,\n","    max_length = 25,\n","    pad_to_max_length=True,\n","    truncation=True\n",")\n","\n","# tokenize and encode sequences in the validation set\n","tokens_val = tokenizer.batch_encode_plus(\n","    val_text,\n","    max_length = 25,\n","    pad_to_max_length=True,\n","    truncation=True\n",")\n","\n","# tokenize and encode sequences in the test set\n","tokens_test = tokenizer.batch_encode_plus(\n","    test_text,\n","    max_length = 25,\n","    pad_to_max_length=True,\n","    truncation=True\n",")\n","\n","#print(tokens_test)\n","\n","train_seq = torch.tensor(tokens_train['input_ids'])\n","train_mask = torch.tensor(tokens_train['attention_mask'])\n","train_y = torch.tensor(train_labels)\n","\n","val_seq = torch.tensor(tokens_val['input_ids'])\n","val_mask = torch.tensor(tokens_val['attention_mask'])\n","val_y = torch.tensor(val_labels)\n","print(val_y)\n","test_seq = torch.tensor(tokens_test['input_ids'])\n","test_mask = torch.tensor(tokens_test['attention_mask'])\n","test_y = torch.tensor(test_labels)\n","\n","\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#define a batch size\n","batch_size = 32\n","\n","# wrap tensors\n","train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","# sampler for sampling the data during training\n","train_sampler = RandomSampler(train_data)\n","\n","# dataLoader for train set\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# wrap tensors\n","val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","# sampler for sampling the data during training\n","val_sampler = SequentialSampler(val_data)\n","\n","# dataLoader for validation set\n","val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n","\n","\n","for param in bert.parameters():\n","    param.requires_grad = False\n","\n","class BERT_Arch(nn.Module):\n","    def __init__(self, bert):\n","        \n","        super(BERT_Arch, self).__init__()\n","\n","        self.bert = bert \n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.1)\n","        \n","        # relu activation function\n","        self.relu =  nn.ReLU()\n","\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768,512)\n","        \n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512,2)\n","\n","        #softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","\n","        #pass the inputs to the model  \n","        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n","        \n","        x = self.fc1(cls_hs)\n","\n","        x = self.relu(x)\n","\n","        x = self.dropout(x)\n","\n","        # output layer\n","        x = self.fc2(x)\n","        \n","        # apply softmax activation\n","        x = self.softmax(x)\n","\n","        return x\n","\n","\n","model = BERT_Arch(bert)\n","\n","# push the model to GPU\n","# model = model.to(device)\n","\n","from transformers import AdamW\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(),\n","                  lr = 1e-5)       \n","\n","\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","\n","class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n","\n","print(\"Class Weights:\",class_weights)\n","\n","weights= torch.tensor(class_weights,dtype=torch.float)\n","\n","cross_entropy  = nn.NLLLoss(weight=weights) \n","\n","epochs = 2\n","\n","def train():\n","  model.train()\n","  total_loss, total_accuracy = 0, 0\n","  total_preds=[]\n","  for step,batch in enumerate(train_dataloader):\n","    if step % 50 == 0 and not step == 0:\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))  \n","    sent_id, mask, labels = batch\n","    model.zero_grad()        \n","    preds = model(sent_id, mask)\n","\n","    loss = cross_entropy(preds, labels)\n","    total_loss = total_loss + loss.item()\n","\n","    # backward pass to calculate the gradients\n","    loss.backward()\n","\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    # update parameters\n","    optimizer.step()\n","\n","    # model predictions are stored on GPU. So, push it to CPU\n","    preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","\n","  # compute the training loss of the epoch\n","  avg_loss = total_loss / len(train_dataloader)\n","  \n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  #returns the loss and predictions\n","  return avg_loss, total_preds\n","\n","\n","def evaluate():\n","      \n","  print(\"\\nEvaluating...\")\n","  \n","  # deactivate dropout layers\n","  model.eval()\n","\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save the model predictions\n","  total_preds = []\n","\n","  # iterate over batches\n","  for step,batch in enumerate(val_dataloader):\n","    \n","    # Progress update every 50 batches.\n","    if step % 50 == 0 and not step == 0:\n","      \n","      # Calculate elapsed time in minutes.\n","      elapsed = format_time(time.time() - t0)\n","            \n","      # Report progress.\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","    # # push the batch to gpu\n","    # batch = [t.to(device) for t in batch]\n","\n","    sent_id, mask, labels = batch\n","\n","    # deactivate autograd\n","    with torch.no_grad():\n","      \n","      # model predictions\n","      preds = model(sent_id, mask)\n","\n","      # compute the validation loss between actual and predicted values\n","      loss = cross_entropy(preds,labels)\n","\n","      total_loss = total_loss + loss.item()\n","\n","      preds = preds.detach().cpu().numpy()\n","\n","      total_preds.append(preds)\n","\n","  # compute the validation loss of the epoch\n","  avg_loss = total_loss / len(val_dataloader) \n","\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  return avg_loss, total_preds\n","\n","\n","best_valid_loss = float('inf')\n","\n","# empty lists to store training and validation loss of each epoch\n","train_losses=[]\n","valid_losses=[]\n","\n","#for each epoch\n","for epoch in range(epochs):\n","     \n","    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","    \n","    #train model\n","    train_loss, _ = train()\n","    \n","    #evaluate model\n","    valid_loss, _ = evaluate()\n","    \n","    #save the best model\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'saved_weights.pt')\n","    \n","    # append training and validation loss\n","    train_losses.append(train_loss)\n","    valid_losses.append(valid_loss)\n","    \n","    print(f'\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')\n","\n","path = 'saved_weights.pt'\n","model.load_state_dict(torch.load(path))\n","\n","with torch.no_grad():\n","  preds = model(test_seq.to(device), test_mask.to(device))\n","  preds = preds.detach().cpu().numpy()\n","\n","\n","preds = np.argmax(preds, axis = 1)\n","print(classification_report(test_y, preds))"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-93eaf805eb16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]}]}