{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBvK0DbLHWDu",
        "outputId": "eb4ecce1-8947-4d3a-997b-368639ff8651"
      },
      "source": [
        "import pickle\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "!pip install transformers==3\n",
        "import os\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import re\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.1.95)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8NhIZTJn2Y9",
        "outputId": "1a6e0ad6-99cf-43c3-945f-a76317037573"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZQ_QhKT7eSN"
      },
      "source": [
        "demo_folder='ottawa'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6iZjLDP_s38",
        "outputId": "1f7e6e2b-a652-4378-ebaf-4dec02ed7a00"
      },
      "source": [
        "files='/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/demo/Tweet_data/'+demo_folder\n",
        "folds = sorted(os.listdir(files))#id\n",
        "new_path = os.path.join(files, folds[0])\n",
        "ids = sorted(os.listdir(new_path)) \n",
        "newfolds = [i for i in ids if i[0] != '.']\n",
        "id_data = newfolds\n",
        "text = ''\n",
        "full_replies_text = []\n",
        "full_tweet_text = []\n",
        "source_path = os.path.join(new_path,'source-tweet')\n",
        "source_file_name = sorted(os.listdir(source_path)) \n",
        "name = [i for i in source_file_name if i[0] != '.']\n",
        "source_file = open(os.path.join(source_path, name[0]))\n",
        "print(os.path.join(source_path, name[0]))\n",
        "for line in source_file:\n",
        "  source_tweet = json.loads(line)\n",
        "  text = text+source_tweet['text']\n",
        "  print(\"SOURCE-TWEET:\",source_tweet['text'])\n",
        "full_tweet_text.append(text)\n",
        "final_path = os.path.join(new_path,'replies')\n",
        "tweets = sorted(os.listdir(final_path))\n",
        "for tweet in tweets:\n",
        "  with open(os.path.join(final_path, tweet)) as f:\n",
        "      for line in f:\n",
        "          tw = json.loads(line)\n",
        "          print(\"REPLY:\",tw['text'])\n",
        "          text = text+tw['text']\n",
        "full_replies_text.append(text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/demo/Tweet_data/ottawa/525032872647065600/source-tweet/525032872647065600.json\n",
            "SOURCE-TWEET: BREAKING: Michael Zehaf-Bebeau had been designated \"high-risk traveller\" by CDN govt, which confiscated his passport http://t.co/dPeMQ78jzm\n",
            "REPLY: @20committee @BlogsofWar How come we have to pussy foot around terrorism for hours before calling what it is.\n",
            "REPLY: @20committee Pathetic.\n",
            "REPLY: @20committee @kshaidle maybe we should start revoking passports AFTER they leave...\n",
            "REPLY: If Michael Zehaf-Bebeau had his passport confiscated doesn't it make sense to track his movements - was this done? @20committee @BlogsofWar\n",
            "REPLY: @aenk963 @BlogsofWar yes, but CSIS can't watch that many 24/7\n",
            "REPLY: Yes:) except in Australia there's a discussion to let 'sympathizers' reenter OZ to rehabilitate! WTF! @MarkOkanagan @20committee @kshaidle\n",
            "REPLY: @20committee @sqwerin and @BarackObama still refuses to call it terrorism!\n",
            "REPLY: micro chip them - Just an uncivil thought :) @20committee @BlogsofWar\n",
            "REPLY: @20committee @AresXtra They must put high risk travelers in civilian detaine camps to prevent attacks.US does this with suspect japs in WWII\n",
            "REPLY: @20committee @kshaidle It makes more sense to let them leave, and invalidate their passports before they return.\n",
            "REPLY: @20committee the incident yesterday (car) in Canada was that a terrorist attack as well?\n",
            "REPLY: @aniekan_o sure looks like one\n",
            "REPLY: @20committee Oh, so they already know where Z-B had gone to? Any travels to the Levant recently?\n",
            "REPLY: @chrisjefferson no info yet\n",
            "REPLY: @20committee   A high risk traveller!  But a risk to #Canadians and government officials , he was okay ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4xRdrMHAd7d"
      },
      "source": [
        "\n",
        "def drop_emoji(x):\n",
        "    x= re.sub(r\"http\\S+\", \"\", x)\n",
        "    x=re.sub(r\"[^\\w\\d\\!\\?\\s]+\",'',x)\n",
        "    drop_list = []\n",
        "    for i,_x in enumerate(x):\n",
        "        if ord(_x) >= 128:\n",
        "            drop_list.append(i)\n",
        "    new_x = []\n",
        "    for i,_x in enumerate(x):\n",
        "        if i not in drop_list:\n",
        "            new_x.append(_x)\n",
        "    return ''.join(new_x)\n",
        "tweets = list(map(drop_emoji,full_replies_text))\n",
        "# print(tweets)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II-yUPRJVGy3",
        "outputId": "72aa3c51-328b-4a97-8ea8-7a797990750a"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(tweets)\n",
        "file1='/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/demo/Tweetstructs/ottawa.json'\n",
        "f=open(file1)\n",
        "tweets=list(map(drop_emoji,full_tweet_text))\n",
        "tweet_dict=json.load(f)\n",
        "tweets_dict=[]\n",
        "tweets_dict.append(tweet_dict)\n",
        "v = DictVectorizer(sparse=False)\n",
        "tweets_x=v.fit_transform(tweets_dict)\n",
        "\n",
        "# print(tweets)\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    tweets,\n",
        "    max_length = 100,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=False,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "# test_y = torch.tensor(test_labels)\n",
        "\n",
        "\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        \n",
        "        super(BERT_Arch, self).__init__()\n",
        "\n",
        "        self.bert = bert \n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "        \n",
        "        # dense layer 2 (Output layer)\n",
        "        self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "        #softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "        #pass the inputs to the model  \n",
        "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
        "        \n",
        "        x = self.fc1(cls_hs)\n",
        "\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # output layer\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "classes=[\"Non-Rumour\",\"Rumour\"]\n",
        "model = BERT_Arch(bert)\n",
        "path = '/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/code/bert_saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path),strict=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "  # preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  pred_logit=model(test_seq.to(device), test_mask.to(device))\n",
        "  results = torch.softmax(pred_logit, dim=1).tolist()[0]\n",
        "  # print(pred_logit)\n",
        "  preds = pred_logit.detach().cpu().numpy()\n",
        "  # print(preds)\n",
        "\n",
        "# for i in range(len(classes)):\n",
        "    # print(f\"{classes[i]}: {round(results[i] * 100)}%\")\n",
        "\n",
        "prob_nr=results[0]\n",
        "prob_r=results[1]\n",
        "pred_text=\"\"\n",
        "preds =list(np.argmax(preds, axis = 1))\n",
        "# print(preds)\n",
        "for i in range(len(preds)):\n",
        "    if(preds[i]==0):\n",
        "        preds[i]=\"Non-Rumour\"\n",
        "        pred_text=preds[i]\n",
        "        \n",
        "    else:\n",
        "        preds[i]=\"Rumour\"\n",
        "        pred_text=preds[i]\n",
        "    \n",
        "# print(classification_report(test_y, preds))\n",
        "message =\"Detection via transformer (text-based) model\"\n",
        "print(message.center(40,'~'))\n",
        "for i in range(len(full_replies_text)):\n",
        "    print(i+1,full_replies_text,'\\n',preds[i].center(40,' '))\n",
        "\n",
        "diff1=abs(prob_nr-prob_r)\n",
        "prob_nr_text=prob_nr\n",
        "prob_r_text=prob_r"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detection via transformer (text-based) model\n",
            "1 ['BREAKING: Michael Zehaf-Bebeau had been designated \"high-risk traveller\" by CDN govt, which confiscated his passport http://t.co/dPeMQ78jzm@20committee @BlogsofWar How come we have to pussy foot around terrorism for hours before calling what it is.@20committee Pathetic.@20committee @kshaidle maybe we should start revoking passports AFTER they leave...If Michael Zehaf-Bebeau had his passport confiscated doesn\\'t it make sense to track his movements - was this done? @20committee @BlogsofWar@aenk963 @BlogsofWar yes, but CSIS can\\'t watch that many 24/7Yes:) except in Australia there\\'s a discussion to let \\'sympathizers\\' reenter OZ to rehabilitate! WTF! @MarkOkanagan @20committee @kshaidle@20committee @sqwerin and @BarackObama still refuses to call it terrorism!micro chip them - Just an uncivil thought :) @20committee @BlogsofWar@20committee @AresXtra They must put high risk travelers in civilian detaine camps to prevent attacks.US does this with suspect japs in WWII@20committee @kshaidle It makes more sense to let them leave, and invalidate their passports before they return.@20committee the incident yesterday (car) in Canada was that a terrorist attack as well?@aniekan_o sure looks like one@20committee Oh, so they already know where Z-B had gone to? Any travels to the Levant recently?@chrisjefferson no info yet@20committee   A high risk traveller!  But a risk to #Canadians and government officials , he was okay ?'] \n",
            "                  Rumour                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vf2fwnsVIRp",
        "outputId": "6ea5f39e-947b-4932-a197-985319058064"
      },
      "source": [
        "filename = '/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/code/rf_tree (1).pkl'\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "# print(tweets_x)\n",
        "preds=loaded_model.predict(tweets_x)\n",
        "y_pred=loaded_model.predict_proba(tweets_x)\n",
        "# print(y_pred)\n",
        "prob_nr=y_pred[0][0]\n",
        "prob_r=y_pred[0][1]\n",
        "pred_graph=\"\"\n",
        "message =\"Detection via Graph model\"\n",
        "print(message.center(40,'~'))\n",
        "for pred in preds:\n",
        "  if(pred==0):\n",
        "    pred_graph=\"Non-Rumour\"\n",
        "  else:\n",
        "    pred_graph=\"Rumour\"\n",
        "print(pred_graph.center(40,' '))\n",
        "diff2=abs(prob_nr-prob_r)\n",
        "prob_nr_graph=prob_nr\n",
        "prob_r_graph=prob_r"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~~~~~~~Detection via Graph model~~~~~~~~\n",
            "                 Rumour                 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTf3Z4p-Vwl4",
        "outputId": "1b434c3e-dced-447e-a237-5ee2ca21368c"
      },
      "source": [
        "sum1=(-1*(prob_nr_text)+prob_r_text)*(0.71)\n",
        "sum2=(-1*(prob_nr_graph)+prob_r_graph)*(0.73)\n",
        "final_sum=sum1+sum2\n",
        "if(final_sum<0):\n",
        "  detection=\"Non-Rumour\"\n",
        "  message=\"Final Detection using joint model: Non-Rumour\"\n",
        "else:\n",
        "  detection=\"Rumour\"\n",
        "  message=\"Final Detection using joint model: Rumour\"\n",
        "print(message.center(40,\"`\"))\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Detection using joint model: Rumour\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7aXur65X-rk"
      },
      "source": [
        "if(detection==\"Non-Rumour\"):\n",
        "  exit(0)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h83UwSyeE2zJ"
      },
      "source": [
        "exit()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjKqqXj8E516",
        "outputId": "9778e3be-da3b-4502-f8f2-fa6f3e57d104"
      },
      "source": [
        "!pip3 install torch==1.0.1 -f https://download.pytorch.org/whl/cpu/stable \n",
        "!git clone https://github.com/huggingface/torchMoji\n",
        "import os\n",
        "os.chdir('torchMoji')\n",
        "!pip3 install -e .\n",
        "#if you restart the package, the notebook risks to crash on a loop\n",
        "#I did not restart and worked fine\n",
        "!python3 scripts/download_weights.py"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cpu/stable\n",
            "Requirement already satisfied: torch==1.0.1 in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "fatal: destination path 'torchMoji' already exists and is not an empty directory.\n",
            "Obtaining file:///content/torchMoji\n",
            "Requirement already satisfied: emoji==0.4.5 in /usr/local/lib/python3.7/dist-packages (from torchmoji==1.0) (0.4.5)\n",
            "Collecting numpy==1.13.1\n",
            "  Using cached https://files.pythonhosted.org/packages/c0/3a/40967d9f5675fbb097ffec170f59c2ba19fc96373e73ad47c2cae9a30aed/numpy-1.13.1.zip\n",
            "Collecting scipy==0.19.1\n",
            "  Using cached https://files.pythonhosted.org/packages/52/67/d9ef9b5058d4a9e3f0ae641ec151790622cbeb37f157de5773358e2bf3da/scipy-0.19.1.tar.gz\n",
            "Collecting scikit-learn==0.19.0\n",
            "  Using cached https://files.pythonhosted.org/packages/c2/38/e3b0333e661ab411545583cb24940223917fe7ffc9c68a77730dce3b10b0/scikit-learn-0.19.0.tar.gz\n",
            "Collecting text-unidecode==1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/09/c6/c7a477228b2162937f200ece3793bb21c0f21f66b00fc010cdeb93cf465b/text_unidecode-1.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: numpy, scipy, scikit-learn\n",
            "  Building wheel for numpy (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for numpy\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for numpy\n",
            "\u001b[31m  ERROR: Failed cleaning build dir for numpy\u001b[0m\n",
            "  Building wheel for scipy (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scipy\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for scipy\n",
            "\u001b[31m  ERROR: Failed cleaning build dir for scipy\u001b[0m\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for scikit-learn\n",
            "Failed to build numpy scipy scikit-learn\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scipy>=1.0.0, but you'll have scipy 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2021.4.8 has requirement numpy>=1.15.1, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.1 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.1 has requirement scipy>=1.0, but you'll have scipy 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pywavelets 1.1.1 has requirement numpy>=1.13.3, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: python-slugify 4.0.1 has requirement text-unidecode>=1.3, but you'll have text-unidecode 1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyerfa 1.7.2 has requirement numpy>=1.16, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.5 has requirement numpy>=1.15.4, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: opencv-python 4.1.2.30 has requirement numpy>=1.14.5, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: opencv-contrib-python 4.1.2.30 has requirement numpy>=1.14.5, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.51.2 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: librosa 0.8.0 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: librosa 0.8.0 has requirement scikit-learn!=0.19.0,>=0.14.0, but you'll have scikit-learn 0.19.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: librosa 0.8.0 has requirement scipy>=1.0.0, but you'll have scipy 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jaxlib 0.1.65+cuda110 has requirement numpy>=1.16, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 has requirement scikit-learn>=0.19.1, but you'll have scikit-learn 0.19.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement scipy>=1.1.0, but you'll have scipy 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, scipy, scikit-learn, text-unidecode, torchmoji\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "    Running setup.py install for numpy ... \u001b[?25l\u001b[?25herror\n",
            "  Rolling back uninstall of numpy\n",
            "  Moving to /usr/bin/f2py\n",
            "   from /tmp/pip-uninstall-ne_69544/f2py\n",
            "  Moving to /usr/local/bin/f2py\n",
            "   from /tmp/pip-uninstall-_tvr4y76/f2py\n",
            "  Moving to /usr/local/bin/f2py3\n",
            "   from /tmp/pip-uninstall-_tvr4y76/f2py3\n",
            "  Moving to /usr/local/bin/f2py3.7\n",
            "   from /tmp/pip-uninstall-_tvr4y76/f2py3.7\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/numpy-1.19.5.dist-info/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~umpy-1.19.5.dist-info\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/numpy.libs/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~umpy.libs\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/numpy/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~umpy\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-da32h7cd/numpy/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-da32h7cd/numpy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-sd3d0l4m/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Weight file already exists at model/pytorch_model.bin. Would you like to redownload it anyway? [y/n]\n",
            "y\n",
            "About to download the pretrained weights file from https://www.dropbox.com/s/q8lax9ary32c7t9/pytorch_model.bin?dl=0#\n",
            "Downloading...\n",
            "Running system call: wget https://www.dropbox.com/s/q8lax9ary32c7t9/pytorch_model.bin?dl=0# -O /content/torchMoji/model/pytorch_model.bin\n",
            "--2021-05-02 18:28:48--  https://www.dropbox.com/s/q8lax9ary32c7t9/pytorch_model.bin?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/q8lax9ary32c7t9/pytorch_model.bin [following]\n",
            "--2021-05-02 18:28:48--  https://www.dropbox.com/s/raw/q8lax9ary32c7t9/pytorch_model.bin\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce6827e8a906900d5690501c121.dl.dropboxusercontent.com/cd/0/inline/BNt8qI-KTPLQsCU13yDHVPTao7N4qMVhKOGgBMSh-5O9xVC_pvjwSlvTBRwPoRa0obeM0ygYRAiPj6L651lgBtG0sZ48RRQ2_bUZeYvhgBtoRfomzj91fdLvC2d_5Tx3ORIy4QwdMmJDFY9ytQ-91AuY/file# [following]\n",
            "--2021-05-02 18:28:49--  https://uce6827e8a906900d5690501c121.dl.dropboxusercontent.com/cd/0/inline/BNt8qI-KTPLQsCU13yDHVPTao7N4qMVhKOGgBMSh-5O9xVC_pvjwSlvTBRwPoRa0obeM0ygYRAiPj6L651lgBtG0sZ48RRQ2_bUZeYvhgBtoRfomzj91fdLvC2d_5Tx3ORIy4QwdMmJDFY9ytQ-91AuY/file\n",
            "Resolving uce6827e8a906900d5690501c121.dl.dropboxusercontent.com (uce6827e8a906900d5690501c121.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uce6827e8a906900d5690501c121.dl.dropboxusercontent.com (uce6827e8a906900d5690501c121.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BNtxSM8TK0xZcqxRW7Jlkq7sDI3dHLKA1Qzyo4TN_JSgRBVZjaR0R2Qr5M1V5G690wp0R91QN4lf966LsafxLG0pPFa38IWyKeT9K0BecXNqf7UNjWwny-lOynSkgNCVWP8Yxkt8-GfbbIRdoP2tB7ycUceiZ6dqHQ392WOoFww1O6cFTEw8rPkEJkgkOi0ya5MGWVWwgGNCyCLIhp6NaxwiQLBoZgV6LJV9skBDElQZRWrtJlw9Q-rOI0hS40jrGvMEe4kYqUvTvf0SM1ICzhAYSltOWOYu3mIQaW2lK1c_JBcf-_wppR52gkONH6Y9M42tUl8UcQRVJJi1yd-a40MbBHGtcwUlHVde2njIBxWzNW0PneRC33cf7Pw8ejsbLbw/file [following]\n",
            "--2021-05-02 18:28:49--  https://uce6827e8a906900d5690501c121.dl.dropboxusercontent.com/cd/0/inline2/BNtxSM8TK0xZcqxRW7Jlkq7sDI3dHLKA1Qzyo4TN_JSgRBVZjaR0R2Qr5M1V5G690wp0R91QN4lf966LsafxLG0pPFa38IWyKeT9K0BecXNqf7UNjWwny-lOynSkgNCVWP8Yxkt8-GfbbIRdoP2tB7ycUceiZ6dqHQ392WOoFww1O6cFTEw8rPkEJkgkOi0ya5MGWVWwgGNCyCLIhp6NaxwiQLBoZgV6LJV9skBDElQZRWrtJlw9Q-rOI0hS40jrGvMEe4kYqUvTvf0SM1ICzhAYSltOWOYu3mIQaW2lK1c_JBcf-_wppR52gkONH6Y9M42tUl8UcQRVJJi1yd-a40MbBHGtcwUlHVde2njIBxWzNW0PneRC33cf7Pw8ejsbLbw/file\n",
            "Reusing existing connection to uce6827e8a906900d5690501c121.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89616062 (85M) [application/octet-stream]\n",
            "Saving to: ‘/content/torchMoji/model/pytorch_model.bin’\n",
            "\n",
            "/content/torchMoji/ 100%[===================>]  85.46M  52.6MB/s    in 1.6s    \n",
            "\n",
            "2021-05-02 18:28:51 (52.6 MB/s) - ‘/content/torchMoji/model/pytorch_model.bin’ saved [89616062/89616062]\n",
            "\n",
            "Downloaded weights to model/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR3AKGtYFGOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39694539-5c32-4abc-c8cc-63b20c33dc57"
      },
      "source": [
        "import numpy as np\n",
        "import emoji, json\n",
        "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH\n",
        "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
        "from torchmoji.model_def import torchmoji_emojis\n",
        "import statistics\n",
        "from statistics import mode\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "demo_folder = 'ottawa'\n",
        "EMOJIS = \":joy: :unamused: :weary: :sob: :heart_eyes: :pensive: :ok_hand: :blush: :heart: :smirk: :grin: :notes: :flushed: :100: :sleeping: :relieved: :relaxed: :raised_hands: :two_hearts: :expressionless: :sweat_smile: :pray: :confused: :kissing_heart: :heartbeat: :neutral_face: :information_desk_person: :disappointed: :see_no_evil: :tired_face: :v: :sunglasses: :rage: :thumbsup: :cry: :sleepy: :yum: :triumph: :hand: :mask: :clap: :eyes: :gun: :persevere: :smiling_imp: :sweat: :broken_heart: :yellow_heart: :musical_note: :speak_no_evil: :wink: :skull: :confounded: :smile: :stuck_out_tongue_winking_eye: :angry: :no_good: :muscle: :facepunch: :purple_heart: :sparkling_heart: :blue_heart: :grimacing: :sparkles:\".split(' ')\n",
        "stance={\"support\":[\"facepunch\",\"sweat_smile\",\"stuck_out_tongue_winking_eye\",\"see_no_evil\",\"pray\",\"eyes\",\"information_desk_person\",\"wink\",\"joy\",\"yum\",\"clap\",\"sparkles\",\"heart_eyes\",\"ok_hand\",\"blush\",\"heart\",\"flushed\",\"sparkling_heart\",\"100\",\"relieved\",\"muscle\",\"hand\",\"raised_hands\",\"purple_heart\",\"two_hearts\",\"kissing_heart\",\"blue_heart\",\"v\",\"heartbeat\",\"sunglasses\",\"thumbsup\",\"smile\",\"yellow_heart\",\"neutral_face\",\"pensive\"],\"deny\":[\"sob\",\"gun\",\"triumph\",\"confounded\",\"grin\",\"cry\",\"disappointed\",\"tired_face\",\"rage\",\"sleepy\",\"smiling_imp\",\"sweat\",\"broken_heart\",\"confused\",\"skull\",\"angry\",\"no_good\",\"grimacing\",\"expressionless\",\"unamused\",\"speak_no_evil\",\"smirk\",\"weary\",\"confounded\"]}\n",
        "emotions={\"happy\":[\"sparkles\",\"speak_no_evil\",\"sparkling_heart\",\"blue_heart\",\"facepunch\",\"purple_heart\",\"joy\",\"muscle\",\"musical_note\",\"heart_eyes\",\"ok_hand\",\"blush\",\"heart\",\"yum\",\"notes\",\"100\",\"relaxed\",\"yellow_heart\",\"raised_hands\",\"two_hearts\",\"kissing_heart\",\"heartbeat\",\"see_no_evil\",\"thumbsup\",\"v\",\"sunglasses\",\"triumph\",\"clap\",\"smile\"],\"sad\":[\"sob\",\"disappointed\",\"sad\",\"tired_face\",\"cry\",\"persevere\",\"broken_heart\",\"confounded\"],\"angry\":[\"angry\",\"no_good\"\"skull\",\"rage\",\"hand\",\"gun\"],\"anxious\":[\"eyes\",\"confused\",\"sweat_smile\",\"weary\",\"pensive\",\"flushed\",\"pray\",\"sweat\"],\"neutral\":[\"neutral_face\",\"expressionless\",\"unamused\",\"neutral\",\"relieved\",\"information_desk_person\",\"notes\",\"grimacing\"],\"sarcasm\":[\"smirk\",\"grin\",\"sleepy\",\"smiling_imp\",\"wink\",\"stuck_out_tongue_winking_eye\"]}\n",
        "model = torchmoji_emojis('model/pytorch_model.bin')\n",
        "weightage={\"support\":1,\"deny\":1}\n",
        "final_stances=[]\n",
        "emotion_list=[]\n",
        "\n",
        "with open(VOCAB_PATH, 'r') as f:\n",
        "  vocabulary = json.load(f)\n",
        "st = SentenceTokenizer(vocabulary, 30)\n",
        "def deepmojify(sentence,top_n =5):\n",
        "  def top_elements(array, k):\n",
        "    ind = np.argpartition(array, -k)[-k:]\n",
        "    return ind[np.argsort(array[ind])][::-1]\n",
        "  tokenized, _, _ = st.tokenize_sentences([sentence])\n",
        "  prob = model(tokenized)[0]\n",
        "  emoji_ids = top_elements(prob, top_n)\n",
        "  emojis = list(map(lambda x: EMOJIS[x], emoji_ids))\n",
        "  EMO=[]\n",
        "  stances=[]\n",
        "  for e in emojis:\n",
        "    em=e[1:len(e)-1]\n",
        "    # print(em)\n",
        "    for s in stance:\n",
        "      if em in stance[s]:\n",
        "        stances.append(s)\n",
        "    for e in emotions:\n",
        "      if em in emotions[e]:\n",
        "        EMO.append(e)\n",
        "  # print(stances)\n",
        "  stances_dict=dict(Counter(stances))\n",
        "  emo_dict=dict(Counter(EMO))\n",
        "  print(\"EMOTIONS: \",emo_dict)\n",
        "  wt=0\n",
        "  ST=\"\"\n",
        "  weight={\"happy\":1,\"sad\":1,\"anxious\":1,\"angry\":1,\"neutral\":1,\"sarcasm\":1}\n",
        "  for stance_key in stances_dict:\n",
        "    if(stances_dict[stance_key]*weightage[stance_key]>wt):\n",
        "      wt=stances_dict[stance_key]*weightage[stance_key]\n",
        "      ST=stance_key\n",
        "  # print(dict(Counter(stances)))\n",
        "  wt_e=0\n",
        "  for emo_key in emo_dict:\n",
        "    if(emo_dict[emo_key]*weight[emo_key]>wt_e):\n",
        "      wt_e=emo_dict[emo_key]*weight[emo_key]\n",
        "      E=emo_key\n",
        "  # print(mode(stances))\n",
        "  emotion_list.append(E)\n",
        "  final_stances.append(ST)\n",
        "  return emoji.emojize(f\"{sentence} {' '.join(emojis)}\", use_aliases=True)\n",
        "# text = ['Get back to work.']\n",
        "# for _ in text:\n",
        "#   print(deepmojify(_, top_n  = 5))\n",
        "\n",
        "\n",
        "files='/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/demo/Tweet_data/'+demo_folder\n",
        "folds = sorted(os.listdir(files))\n",
        "for folder in folds:\n",
        "  new_path = os.path.join(files, folder)\n",
        "  ids = sorted(os.listdir(new_path)) \n",
        "  newfolds = [i for i in ids if i[0] != '.']\n",
        "  id_data = newfolds\n",
        "  text = []\n",
        "  \n",
        "  final_path = os.path.join(new_path,'replies')\n",
        "  tweets = sorted(os.listdir(final_path))\n",
        "  for tweet in tweets:\n",
        "    with open(os.path.join(final_path, tweet)) as f:\n",
        "        for line in f:\n",
        "            tw = json.loads(line)\n",
        "            # print(\"TEXT:\",tw['text'])\n",
        "            text.append(tw['text'])\n",
        "  # print(\"STANCES\\n\")\n",
        "  for _ in text:\n",
        "    print(deepmojify(_, top_n  = 5))\n",
        "  print(\"\\n\")\n",
        "\n",
        "# print(emotion_list)\n",
        "stance_emoji_count=Counter(final_stances)\n",
        "# print(stance_emoji_count)\n",
        "if(stance_emoji_count[\"support\"]>stance_emoji_count[\"deny\"]):\n",
        "  pred=1\n",
        "elif(stance_emoji_count[\"support\"]<stance_emoji_count[\"deny\"]):\n",
        "  pred=-1\n",
        "else:\n",
        "  pred=0\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/torchMoji/torchmoji/model_def.py:159: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(self.embed.weight.data, a=-0.5, b=0.5)\n",
            "/content/torchMoji/torchmoji/model_def.py:161: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(t)\n",
            "/content/torchMoji/torchmoji/model_def.py:163: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(t)\n",
            "/content/torchMoji/torchmoji/model_def.py:165: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  nn.init.constant(t, 0)\n",
            "/content/torchMoji/torchmoji/model_def.py:167: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(self.output_layer[0].weight.data)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EMOTIONS:  {'angry': 2, 'neutral': 2, 'anxious': 1}\n",
            "@20committee @BlogsofWar How come we have to pussy foot around terrorism for hours before calling what it is. 😡 😠 😑 😐 😕\n",
            "EMOTIONS:  {'angry': 1, 'happy': 1, 'neutral': 3}\n",
            "@20committee Pathetic. ✋ 😂 😒 😑 😐\n",
            "EMOTIONS:  {'sarcasm': 3, 'happy': 1, 'neutral': 1}\n",
            "@20committee @kshaidle maybe we should start revoking passports AFTER they leave... 😉 😜 😏 👍 😬\n",
            "EMOTIONS:  {'happy': 1, 'anxious': 3, 'neutral': 1}\n",
            "If Michael Zehaf-Bebeau had his passport confiscated doesn't it make sense to track his movements - was this done? @20committee @BlogsofWar 😂 😕 👀 😐 😳\n",
            "EMOTIONS:  {'sad': 2, 'anxious': 1, 'neutral': 1, 'happy': 1}\n",
            "@aenk963 @BlogsofWar yes, but CSIS can't watch that many 24/7 😢 😕 😞 😬 🙈\n",
            "EMOTIONS:  {'angry': 2, 'anxious': 1, 'sarcasm': 1, 'neutral': 1}\n",
            "Yes:) except in Australia there's a discussion to let 'sympathizers' reenter OZ to rehabilitate! WTF! @MarkOkanagan @20committee @kshaidle 😡 😠 😳 😜 😬\n",
            "EMOTIONS:  {'angry': 2, 'happy': 1, 'sad': 2}\n",
            "@20committee @sqwerin and @BarackObama still refuses to call it terrorism! 😡 😠 😤 😞 😢\n",
            "EMOTIONS:  {'happy': 5}\n",
            "micro chip them - Just an uncivil thought :) @20committee @BlogsofWar 💓 💛 👍 ❤ 😄\n",
            "EMOTIONS:  {'angry': 2, 'sarcasm': 1, 'anxious': 1, 'happy': 1}\n",
            "@20committee @AresXtra They must put high risk travelers in civilian detaine camps to prevent attacks.US does this with suspect japs in WWII 😡 😠 😈 😕 👍\n",
            "EMOTIONS:  {'happy': 3, 'sarcasm': 1, 'anxious': 1}\n",
            "@20committee @kshaidle It makes more sense to let them leave, and invalidate their passports before they return. 👍 😉 😄 😕 😊\n",
            "EMOTIONS:  {'anxious': 2, 'sad': 2, 'neutral': 1}\n",
            "@20committee the incident yesterday (car) in Canada was that a terrorist attack as well? 😕 😢 😳 😖 😬\n",
            "EMOTIONS:  {'sarcasm': 3, 'happy': 2}\n",
            "@aniekan_o sure looks like one 😉 👍 😏 😜 😄\n",
            "EMOTIONS:  {'sarcasm': 2, 'happy': 1, 'anxious': 2}\n",
            "@20committee Oh, so they already know where Z-B had gone to? Any travels to the Levant recently? 😉 😜 😄 😕 👀\n",
            "EMOTIONS:  {'anxious': 2, 'sad': 2}\n",
            "@chrisjefferson no info yet 😕 👀 😞 🙅 😢\n",
            "EMOTIONS:  {'sarcasm': 1, 'neutral': 1, 'anxious': 2, 'happy': 1}\n",
            "@20committee   A high risk traveller!  But a risk to #Canadians and government officials , he was okay ? 😉 😬 😕 😳 😄\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ-WoCgkIKxy",
        "outputId": "e6af09aa-87bd-4286-9946-1fe18666cec4"
      },
      "source": [
        "!pip install npy-append-array"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: npy-append-array in /usr/local/lib/python3.7/dist-packages (0.9.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8XgdwFnLETn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4bfd0ad-6124-439e-e64b-bc228c9969be"
      },
      "source": [
        "from keras.models import load_model\n",
        "from collections import Counter\n",
        "from npy_append_array import NpyAppendArray\n",
        "import numpy as np\n",
        "def predict_model_veracity():\n",
        "    # x_stance= []\n",
        "    # x_stance.append([x_test,emotion_list])\n",
        "    x_test_ver = np.load('/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/demo/test/test_array.npy')\n",
        "    stance_lstm=predict_model_stance(emotion_list)\n",
        "    model=load_model('/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/code/Stance_Veracity/my_model_veracity.h5')\n",
        "    Y_pred = model.predict_classes(x_test_ver)\n",
        "    labels=[]\n",
        "    true_labels=[]\n",
        "    # print(Y_pred[:10])\n",
        "    # for i in range(len(x_test)):\n",
        "    #     print(\"X=%s, Predicted=%s\" % (x_test[i], Y_pred[i]))\n",
        "    \n",
        "    # print(classification_report(y_test,Y_pred))\n",
        "    preds=np.ndarray.tolist(Y_pred)\n",
        "    for i in range(len(preds)):\n",
        "        if(preds[i]==0):\n",
        "            labels.append(\"true\")\n",
        "        elif(preds[i]==1):\n",
        "            labels.append(\"false\")\n",
        "        else:\n",
        "            labels.append(\"unverified\")\n",
        "\n",
        "    # for i in range(len(y_test)):\n",
        "    #     if(y_test[i]==0):\n",
        "    #         true_labels.append(\"true\")\n",
        "    #     elif(y_test[i]==1):\n",
        "    #         true_labels.append(\"false\")\n",
        "    #     else:\n",
        "    #        true_labels.append(\"unverified\")\n",
        "    # print(Counter(labels))\n",
        "    # print(true_labels)\n",
        "    predicted='Predicted Veracity'\n",
        "    print(predicted.center(40,'*'))\n",
        "    veracity_bilstm=max(labels,key=labels.count)\n",
        "    \n",
        "    \n",
        "    if(pred==0):\n",
        "        veracity_bilstm='Unverified Rumor'\n",
        "    elif(pred==1):\n",
        "        veracity_bilstm='True Rumor'\n",
        "    else:\n",
        "        veracity_bilstm='False Rumor'\n",
        "    print(veracity_bilstm.center(40,'*'))\n",
        "    print('*'*40)\n",
        "    return(veracity_bilstm)\n",
        "    # for i in range(len(x_test)):\n",
        "    #     print(labels[i],end=\"\\n\")\n",
        "\n",
        "def predict_model_stance(emotion_list):\n",
        "    filename='/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/demo/test/test__array.npy'\n",
        "    x_test_stance = np.load(filename)\n",
        "    model=load_model('/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/code/Stance_Veracity/my_model_stance.h5')\n",
        "    emo_list = []\n",
        "    for em in emotion_list:\n",
        "      if em=='happy':\n",
        "        emo_list.append(0.0)\n",
        "      elif em=='sad':\n",
        "        emo_list.append(1.0)\n",
        "      elif em=='anxious':\n",
        "        emo_list.append(2.0)\n",
        "      elif em=='angry':\n",
        "        emo_list.append(3.0)\n",
        "      elif em=='neutral':\n",
        "        emo_list.append(4.0)\n",
        "      else:\n",
        "        emo_list.append(5.0)\n",
        "    emo_array = np.asarray(emo_list).astype('float32')\n",
        "    emo_array = emo_array[:,np.newaxis,np.newaxis]\n",
        "    emo_array = np.pad(emo_array,((0,0),(2,1),(157,156)),'constant')\n",
        "    # print(emo_array.shape)\n",
        "    # print(x_test_stance.shape)\n",
        "    # np.concatenate(x_test_stance,emo_array)\n",
        "\n",
        "    npaa = NpyAppendArray(filename)\n",
        "    npaa.append(emo_array)\n",
        "    # npaa.append(arr2)\n",
        "    # npaa.append(arr2)\n",
        "    Y_pred=model.predict_classes(x_test_stance)\n",
        "    # y_test=np.ndarray.tolist(y_test)\n",
        "    Y_pred= np.ndarray.tolist(Y_pred)\n",
        "    labels=[]\n",
        "    # true_labels=[]\n",
        "    correct=0\n",
        "    for i in range(len(x_test_stance)):\n",
        "        # if(int(y_test[i][1])==Y_pred[i][1]):\n",
        "        #     correct+=1\n",
        "        if(Y_pred[i][1]==0):\n",
        "            labels.append(\"support\")\n",
        "        elif(Y_pred[i][1]==1):\n",
        "            labels.append(\"deny\")\n",
        "        elif(Y_pred[i][1]==2):\n",
        "            labels.append(\"query\")\n",
        "        else:\n",
        "            labels.append(\"comment\")\n",
        "\n",
        "    # for i in range(len(y_test)):\n",
        "        \n",
        "    #     if(y_test[i][1]==0):\n",
        "    #         true_labels.append(\"support\")\n",
        "    #     elif(y_test[i][1]==1):\n",
        "    #         true_labels.append(\"deny\")\n",
        "    #     elif(y_test[i][1]==2):\n",
        "    #         true_labels.append(\"query\")\n",
        "    #     else:\n",
        "    #         true_labels.append(\"comment\")\n",
        "\n",
        "    # print(\"predicted\")\n",
        "    # for i in range(len(x_test)):\n",
        "    #     print(labels[i],end=\"\\n\")\n",
        "    # print(\"Correct predictions\",correct)\n",
        "    # print(\"Total number if tweets considered\", len(x_test))\n",
        "    # print(\"Accuracy is \", correct/len(x_test))\n",
        "    # print(labels)\n",
        "    # print(true_labels)\n",
        "    stance_lstm_count=Counter(labels)\n",
        "    if(stance_lstm_count[\"support\"]>stance_lstm_count[\"deny\"]):\n",
        "      veracity_lstm=\"true\"\n",
        "    elif(stance_lstm_count[\"support\"]<stance_lstm_count[\"deny\"]):\n",
        "      veracity_lstm=\"false\"\n",
        "    else:\n",
        "      veracity_lstm=\"unverified\"\n",
        "\n",
        "    return(veracity_lstm)\n",
        "\n",
        "    \n",
        "\n",
        "# x_test_ver = np.load('/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/demo/test/test_array.npy')\n",
        "# y_test_ver = np.load('/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/code/Stance_Veracity/test/labels.npy')\n",
        "# print(len(x_test_ver))\n",
        "# print(y_test_ver[:10])\n",
        "\n",
        "# x_test_stance = np.load('/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/demo/test/test_array.npy')\n",
        "# y_test_stance = np.load('/content/drive/MyDrive/CAPSTONE PROJECT - PRAJNA SIRISHA SUKANYA/code/Stance_Veracity/test/fold_stance_labels.npy')\n",
        "# print(len(x_test_stance))\n",
        "# print(y_test_stance[:10])\n",
        "# stance_lstm=predict_model_stance(x_test_stance)\n",
        "veracity_bilstm=predict_model_veracity()\n",
        "# print(veracity)\n",
        "# veracities=[veracity_emoji,veracity_lstm, veracity_bilstm]\n",
        "# print(veracities)\n",
        "# veracity_final=max(veracities,key=veracities.count)\n",
        "# print(\"FINAL PREDICTED VERACITY:\",veracity_final)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "***********Predicted Veracity***********\n",
            "***************True Rumor***************\n",
            "****************************************\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}